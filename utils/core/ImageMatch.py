import logging

import cv2
import numpy as np
import os
import time
import pickle
import json
import shutil
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor


def cv_imread(file_path):
    """è¯»å–å›¾åƒï¼ˆæ”¯æŒä¸­æ–‡è·¯å¾„ï¼‰å¹¶è½¬æ¢ä¸ºç°åº¦å›¾"""
    cv_img = cv2.imdecode(np.fromfile(file_path, dtype=np.uint8), -1)
    return cv2.cvtColor(cv_img, cv2.COLOR_RGB2GRAY)


def keypoint_to_dict(kp):
    """å°†å…³é”®ç‚¹å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸"""
    return {
        'pt': kp.pt,
        'size': kp.size,
        'angle': kp.angle,
        'response': kp.response,
        'octave': kp.octave,
        'class_id': kp.class_id
    }


def dict_to_keypoint(kp_dict):
    """å°†å­—å…¸è½¬æ¢å›å…³é”®ç‚¹å¯¹è±¡"""
    kp = cv2.KeyPoint()
    kp.pt = tuple(kp_dict['pt'])
    kp.size = kp_dict['size']
    kp.angle = kp_dict['angle']
    kp.response = kp_dict['response']
    kp.octave = kp_dict['octave']
    kp.class_id = kp_dict['class_id']
    return kp


class ImageMatch:
    def __init__(self, mode='match', config=None, cache_file="feature_cache.pkl"):
        """
        å›¾åƒåŒ¹é…å™¨

        :param mode: å·¥ä½œæ¨¡å¼ - 'train' æˆ– 'match'
        :param config: é…ç½®å­—å…¸ï¼Œè®­ç»ƒæ¨¡å¼ä¸‹éœ€è¦æä¾›'database_dir'
        :param cache_file: ç‰¹å¾ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        self.cache_file = cache_file
        self.mode = mode
        self.config = config or {}

        # åˆå§‹åŒ–SIFTæ£€æµ‹å™¨å’ŒFLANNåŒ¹é…å™¨
        self.sift = cv2.SIFT_create()

        flann_index_kdtree = 1
        index_params = dict(algorithm=flann_index_kdtree, trees=5)
        search_params = dict(checks=50)
        self.flann = cv2.FlannBasedMatcher(index_params, search_params)

        if mode == 'match':
            self._load_cache()
        elif mode == 'train':
            if 'database_dir' not in self.config:
                raise ValueError("è®­ç»ƒæ¨¡å¼éœ€è¦æä¾› 'database_dir' é…ç½®")
            self.database = self._extract_features()
            self._save_cache()
        else:
            raise ValueError(f"æ— æ•ˆæ¨¡å¼: {mode}. å¿…é¡»æ˜¯ 'train' æˆ– 'match'")

    def _extract_features(self):
        """æå–æ•°æ®åº“ç‰¹å¾"""
        database_dir = self.config['database_dir']
        min_samples = self.config.get('min_samples_per_class', 1)
        num_workers = self.config.get('num_workers', 4)

        database = defaultdict(list)
        self.logger.info(f"â³ æ­£åœ¨æå–ç‰¹å¾: {database_dir}")
        start_time = time.time()

        # è·å–æ‰€æœ‰ç±»åˆ«ç›®å½•
        class_dirs = [d for d in os.listdir(database_dir)
                      if os.path.isdir(os.path.join(database_dir, d))]

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = []
            for class_name in class_dirs:
                class_dir = os.path.join(database_dir, class_name)
                if not self._has_valid_images(class_dir):
                    self.logger.warning(f"âš ï¸ è·³è¿‡ç©ºç›®å½•: {class_name}")
                    continue
                futures.append(executor.submit(
                    self._process_class_dir, class_dir, class_name, min_samples
                ))

            for future in futures:
                class_name, class_data = future.result()
                if class_data:  # åªæ·»åŠ æœ‰æœ‰æ•ˆæ•°æ®çš„ç±»åˆ«
                    database[class_name] = class_data

        elapsed = time.time() - start_time
        self.logger.info(f"âœ… ç‰¹å¾æå–å®Œæˆ: {len(database)} ä¸ªç±»åˆ«, {sum(len(v) for v in database.values())} ä¸ªæ ·æœ¬, è€—æ—¶: {elapsed:.1f}ç§’")
        return dict(database)

    @staticmethod
    def _preprocess_img(img):
        """
        é¢„å¤„ç†å¢å¼ºç‰¹å¾
        :param img:
        :return:
        """
        # ===== é¢„å¤„ç†å¢å¼º =====
        if len(img.shape) == 3:  # ç¡®ä¿ç°åº¦åŒ–
            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        img = cv2.equalizeHist(img)  # å¯¹æ¯”åº¦å¢å¼º
        img = cv2.GaussianBlur(img, (3, 3), 0)  # é™å™ª
        # # æ˜¾ç¤ºç»“æœå›¾åƒ
        # cv2.imshow("ROI Visualization", img)
        # cv2.waitKey(0)
        # cv2.destroyAllWindows()
        return img

    def _process_class_dir(self, class_dir, class_name, min_samples=1):
        """å¤„ç†å•ä¸ªç±»åˆ«çš„ç›®å½•"""
        class_data = []
        processed = 0

        for filename in sorted(os.listdir(class_dir)):
            if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                continue

            img_path = os.path.join(class_dir, filename)
            if not os.path.exists(img_path):
                self.logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ - {img_path}")
                continue

            try:
                img = cv_imread(img_path)
                if img is None:
                    self.logger.warning(f"âš ï¸ æ— æ³•è¯»å–å›¾åƒ: {img_path}")
                    continue

                kp, des = self.sift.detectAndCompute(self._preprocess_img(img), None)  # ä½¿ç”¨é¢„å¤„ç†åçš„å›¾åƒ

                if des is None or len(des) < 10:  # å¿½ç•¥ç‰¹å¾ç‚¹å¤ªå°‘çš„å›¾åƒ
                    self.logger.warning(f"âš ï¸ ç‰¹å¾ä¸è¶³: {img_path} - {len(kp) if kp else 0}ä¸ªå…³é”®ç‚¹")
                    continue

                # å­˜å‚¨ç‰¹å¾å’Œå…ƒæ•°æ®
                class_data.append({
                    'class_name': class_name,
                    'image_path': img_path,
                    'keypoints': kp,  # åŸå§‹KeyPointå¯¹è±¡
                    'descriptors': des,
                    'filename': filename
                })
                processed += 1

                # é™åˆ¶æ¯ä¸ªç±»åˆ«çš„æœ€å°æ ·æœ¬æ•°
                if min_samples > 0 and processed >= min_samples:
                    break

            except Exception as e:
                self.logger.error(f"âš ï¸ å¤„ç† {img_path} æ—¶å‡ºé”™: {str(e)}")

        if class_data:
            self.logger.info(f"ğŸ·ï¸ ç±»åˆ« '{class_name}' æå–äº† {len(class_data)} ä¸ªæ ·æœ¬")
            return class_name, class_data
        return class_name, None

    def _has_valid_images(self, class_dir):
        """æ£€æŸ¥ç›®å½•æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„å›¾åƒæ–‡ä»¶"""
        for filename in os.listdir(class_dir):
            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                return True
        return False

    def _save_cache(self):
        """ä¿å­˜ç‰¹å¾ç¼“å­˜ - ä½¿ç”¨å®‰å…¨å†™å…¥æ–¹æ³•é¿å…æ–‡ä»¶æŸå"""
        # å‡†å¤‡å¯åºåˆ—åŒ–çš„æ•°æ®åº“
        serializable_db = {}
        for class_name, samples in self.database.items():
            serializable_samples = []
            for sample in samples:
                # è½¬æ¢å…³é”®ç‚¹ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
                serializable_kp = [keypoint_to_dict(kp) for kp in sample['keypoints']]

                serializable_samples.append({
                    'class_name': sample['class_name'],
                    'image_path': sample['image_path'],
                    'keypoints': serializable_kp,  # å¯åºåˆ—åŒ–æ ¼å¼
                    'descriptors': sample['descriptors'],
                    'filename': sample['filename']
                })
            serializable_db[class_name] = serializable_samples

        cache_data = {
            'database': serializable_db,
            'config': self.config,
            'timestamp': time.time()
        }

        try:
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶å†™å…¥ï¼Œé¿å…å†™å…¥è¿‡ç¨‹ä¸­æ–­å¯¼è‡´æ–‡ä»¶æŸå
            temp_file = self.cache_file + ".tmp"

            with open(temp_file, 'wb') as f:
                pickle.dump(cache_data, f)

            # å†™å…¥å®Œæˆåå†é‡å‘½åä¸ºç›®æ ‡æ–‡ä»¶
            if os.path.exists(self.cache_file):
                os.remove(self.cache_file)
            shutil.move(temp_file, self.cache_file)

            file_size = os.path.getsize(self.cache_file) / (1024 * 1024)  # MB
            self.logger.info(f"ğŸ’¾ ç‰¹å¾ç¼“å­˜å·²ä¿å­˜: {self.cache_file} ({file_size:.2f} MB)")
            return True
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {str(e)}")
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file):
                os.remove(temp_file)
            return False

    def _load_cache(self):
        """åŠ è½½ç‰¹å¾ç¼“å­˜"""
        if not os.path.exists(self.cache_file):
            self.logger.error(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {self.cache_file}")
            raise FileNotFoundError(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {self.cache_file}")

        try:
            with open(self.cache_file, 'rb') as f:
                cache_data = pickle.load(f)

            # è½¬æ¢å›åŸå§‹KeyPointæ ¼å¼
            original_db = {}
            for class_name, samples in cache_data['database'].items():
                original_samples = []
                for sample in samples:
                    # è½¬æ¢å›KeyPointå¯¹è±¡
                    original_kp = [dict_to_keypoint(kp_dict) for kp_dict in sample['keypoints']]

                    original_samples.append({
                        'class_name': sample['class_name'],
                        'image_path': sample['image_path'],
                        'keypoints': original_kp,  # åŸå§‹KeyPointå¯¹è±¡
                        'descriptors': sample['descriptors'],
                        'filename': sample['filename']
                    })
                original_db[class_name] = original_samples

            self.database = original_db
            self.config = cache_data.get('config', {})
            timestamp = cache_data.get('timestamp', 0)

            # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
            date_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))
            num_classes = len(self.database)
            num_samples = sum(len(v) for v in self.database.values())

            self.logger.debug(f"åŠ è½½ç‰¹å¾ç¼“å­˜: {self.cache_file}")
            self.logger.debug(f"åˆ›å»ºæ—¶é—´: {date_str}")
            self.logger.debug(f"ç±»åˆ«æ•°é‡: {num_classes}")
            self.logger.debug(f"æ ·æœ¬æ€»æ•°: {num_samples}")
            self.logger.info(f"å›¾åƒåŒ¹é…åº“ç¼“å­˜å·²åŠ è½½...")
            return True
        except Exception as e:
            # å°è¯•æä¾›æ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
            if isinstance(e, EOFError):
                self.logger.error(f"ç¼“å­˜æ–‡ä»¶å·²æŸåæˆ–ä¸ºç©º: {self.cache_file}")
                self.logger.error("è¯·åˆ é™¤ç¼“å­˜æ–‡ä»¶å¹¶é‡æ–°è¿è¡Œè®­ç»ƒæ¨¡å¼")
            else:
                self.logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {str(e)}")
            raise RuntimeError(f"åŠ è½½ç¼“å­˜å¤±è´¥: {str(e)}")

    def match(self, query_img, min_match_count=10, ratio_thresh=0.7,
              aggregation='max', num_workers=4, top_k=5):
        """
        åŒ¹é…æŸ¥è¯¢å›¾åƒ

        :param query_img: æŸ¥è¯¢å›¾åƒ(ç°åº¦å›¾)
        :param min_match_count: æœ€å°åŒ¹é…ç‚¹æ•°é˜ˆå€¼
        :param ratio_thresh: Lowe's ratioæµ‹è¯•é˜ˆå€¼
        :param aggregation: åˆ†æ•°èšåˆç­–ç•¥ ('max', 'avg', 'weighted')
        :param num_workers: å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        :param top_k: è¿”å›å‰Kä¸ªç»“æœ
        :return: åŒ¹é…ç»“æœå­—å…¸
        """
        # print("ğŸ” å¼€å§‹åŒ¹é…...")
        start_time = time.time()
        query_kp, query_des = self.sift.detectAndCompute(self._preprocess_img(query_img), None)  # ä½¿ç”¨é¢„å¤„ç†åçš„å›¾åƒ
        if query_des is None:
            self.logger.warning("âš ï¸ æŸ¥è¯¢å›¾åƒæœªæå–åˆ°ç‰¹å¾")
            return {'success': False, 'error': 'No features detected in query image'}

        # print(f"  æŸ¥è¯¢å›¾åƒæå–åˆ° {len(query_kp)} ä¸ªå…³é”®ç‚¹")

        # å­˜å‚¨æ¯ä¸ªç±»åˆ«çš„åŒ¹é…ç»“æœ
        class_results = {}

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡ŒåŒ¹é…
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            for class_name, class_samples in self.database.items():
                futures = []
                for sample_data in class_samples:
                    futures.append(executor.submit(
                        self._match_single_sample,
                        query_des,
                        sample_data,
                        ratio_thresh
                    ))

                # æ”¶é›†è¯¥ç±»åˆ«çš„æ‰€æœ‰åŒ¹é…ç»“æœ
                sample_results = [future.result() for future in futures]

                # æ ¹æ®èšåˆç­–ç•¥è®¡ç®—ç±»åˆ«åˆ†æ•°
                if aggregation == 'max':
                    class_score = max(score for score, _, _ in sample_results)
                elif aggregation == 'avg':
                    class_score = sum(score for score, _, _ in sample_results) / len(sample_results)
                elif aggregation == 'weighted':
                    total_weight = 0
                    weighted_sum = 0
                    for score, _, _ in sample_results:
                        weight = np.sqrt(score)  # ä½¿ç”¨å¹³æ–¹æ ¹ä½œä¸ºæƒé‡
                        weighted_sum += score * weight
                        total_weight += weight
                    class_score = weighted_sum / total_weight if total_weight > 0 else 0
                else:
                    class_score = max(score for score, _, _ in sample_results)

                # æ‰¾åˆ°è¯¥ç±»åˆ«çš„ä»£è¡¨æ€§æ ·æœ¬
                best_sample_idx = np.argmax([score for score, _, _ in sample_results])
                best_sample_score, best_matches, best_sample_data = sample_results[best_sample_idx]

                class_results[class_name] = {
                    'class_score': class_score,
                    'best_sample': best_sample_data,
                    'matches': best_matches,
                    'best_sample_score': best_sample_score
                }

        # è·å–å‰Kä¸ªåŒ¹é…ç»“æœ
        sorted_classes = sorted(
            class_results.items(),
            key=lambda x: x[1]['class_score'],
            reverse=True
        )[:top_k]

        # å‡†å¤‡ç»“æœ
        result = {
            'success': True,
            'query_features': len(query_kp),
            'min_match_count': min_match_count,
            'ratio_thresh': ratio_thresh,
            'elapsed_time': time.time() - start_time,
            'top_matches': []
        }

        # æ£€æŸ¥æœ€ä½³åŒ¹é…æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
        best_match = sorted_classes[0] if sorted_classes else None
        if best_match and best_match[1]['best_sample_score'] >= min_match_count:
            result['best_match'] = best_match[0]
        else:
            result['best_match'] = None

        # æ„å»ºtop_matchesåˆ—è¡¨
        for i, (class_name, class_data) in enumerate(sorted_classes):
            sample_data = class_data['best_sample']
            match_info = {
                'rank': i + 1,
                'class_name': class_name,
                'class_score': round(class_data['class_score'], 2),
                'best_sample_score': class_data['best_sample_score'],
                'sample_path': sample_data['image_path'],
                'sample_filename': sample_data['filename']
            }
            result['top_matches'].append(match_info)

        # print(f"âœ… åŒ¹é…å®Œæˆ! è€—æ—¶: {result['elapsed_time']:.2f}ç§’")
        # print(f"  æœ€ä½³åŒ¹é…: {result.get('best_match', 'æ— ')}")
        # for match in result['top_matches'][:3]:
        #     print(f"  #{match['rank']} {match['class_name']}: ç±»åˆ«åˆ†={match['class_score']}, æ ·æœ¬åˆ†={match['best_sample_score']}")

        return result

    def _match_single_sample(self, query_des, sample_data, ratio_thresh):
        """åŒ¹é…å•ä¸ªæ•°æ®åº“æ ·æœ¬"""
        db_des = sample_data['descriptors']

        # ä½¿ç”¨FLANNè¿›è¡ŒKNNåŒ¹é…
        matches = self.flann.knnMatch(query_des, db_des, k=2)

        # åº”ç”¨Lowe's ratioæµ‹è¯•ç­›é€‰è‰¯å¥½åŒ¹é…
        good_matches = []
        for match_pair in matches:
            if len(match_pair) < 2:
                continue
            m, n = match_pair
            if m.distance < ratio_thresh * n.distance:
                good_matches.append(m)

        return len(good_matches), good_matches, sample_data

    def match_file(self, query_path, **kwargs):
        """ä»æ–‡ä»¶åŠ è½½æŸ¥è¯¢å›¾åƒå¹¶è¿›è¡ŒåŒ¹é…"""
        query_img = cv_imread(query_path)
        if query_img is None:
            self.logger.warning(f"âš ï¸ æ— æ³•è¯»å–å›¾åƒ: {query_path}")
            return {'success': False, 'error': f'Unable to read image: {query_path}'}

        self.logger.debug(f"ğŸ” åŒ¹é…å›¾åƒ: {os.path.basename(query_path)}")
        return self.match(query_img, **kwargs)

    def get_database_info(self):
        """è·å–æ•°æ®åº“ä¿¡æ¯"""
        if not hasattr(self, 'database'):
            return {'error': 'Database not loaded'}

        num_classes = len(self.database)
        num_samples = sum(len(v) for v in self.database.values())
        class_info = {cls: len(samples) for cls, samples in self.database.items()}

        return {
            'num_classes': num_classes,
            'num_samples': num_samples,
            'classes': class_info
        }

    @staticmethod
    def train(database_dir, cache_file="feature_cache.pkl",
              min_samples_per_class=1, num_workers=4):
        """
        è®­ç»ƒæ¨¡å¼: æå–ç‰¹å¾å¹¶ä¿å­˜åˆ°ç¼“å­˜

        :param database_dir: æ•°æ®åº“å›¾åƒç›®å½•
        :param cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        :param min_samples_per_class: æ¯ä¸ªç±»åˆ«æœ€å°‘æå–çš„æ ·æœ¬æ•°
        :param num_workers: å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        :return: ImageMatchå®ä¾‹
        """
        config = {
            'database_dir': database_dir,
            'min_samples_per_class': min_samples_per_class,
            'num_workers': num_workers
        }
        return ImageMatch(
            mode='train',
            config=config,
            cache_file=cache_file
        )

    @staticmethod
    def from_cache(cache_file="feature_cache.pkl"):
        """
        åŒ¹é…æ¨¡å¼: ç›´æ¥ä»ç¼“å­˜åŠ è½½ç‰¹å¾

        :param cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        :return: ImageMatchå®ä¾‹
        """
        return ImageMatch(
            mode='match',
            cache_file=cache_file
        )


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è®­ç»ƒæ¨¡å¼ï¼šæå–ç‰¹å¾å¹¶ä¿å­˜ç¼“å­˜
    print("===== è®­ç»ƒæ¨¡å¼ =====")
    trainer = ImageMatch.train(
        database_dir="avatar_database",
        cache_file="IM.pkl",
        min_samples_per_class=2,
        num_workers=4
    )

    # è·å–æ•°æ®åº“ä¿¡æ¯
    db_info = trainer.get_database_info()
    print("\næ•°æ®åº“ä¿¡æ¯:")
    print(f"ç±»åˆ«æ•°é‡: {db_info['num_classes']}")
    print(f"æ ·æœ¬æ€»æ•°: {db_info['num_samples']}")
    print("å„ç±»åˆ«æ ·æœ¬æ•°:")
    for cls, count in db_info['classes'].items():
        print(f"  {cls}: {count}ä¸ªæ ·æœ¬")

    # åŒ¹é…æ¨¡å¼ï¼šç›´æ¥ä»ç¼“å­˜åŠ è½½
    print("\n===== åŒ¹é…æ¨¡å¼ =====")
    matcher = ImageMatch.from_cache("IM.pkl")

    # è¿›è¡ŒåŒ¹é…
    query_path = "val/2025-06-16_15-35-52.840365.png"
    result = matcher.match_file(
        query_path,
        min_match_count=15,
        ratio_thresh=0.7,
        aggregation='max',
        num_workers=4,
        top_k=5
    )

    # æ˜¾ç¤ºåŒ¹é…ç»“æœ
    if result['success']:
        print("\nåŒ¹é…ç»“æœæ‘˜è¦:")
        print(f"æœ€ä½³åŒ¹é…: {result.get('best_match', 'æ— ')}")
        print("Top 5åŒ¹é…:")
        for match in result['top_matches']:
            print(f"  #{match['rank']} {match['class_name']}: "
                  f"ç±»åˆ«åˆ†={match['class_score']}, æ ·æœ¬åˆ†={match['best_sample_score']}")
